<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Unknown</title>
    <link>/unknown/</link>
    <description>Recent content on Data Unknown</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/unknown/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Project 1</title>
      <link>/unknown/project/project1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/unknown/project/project1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Raster Time Series Animation</title>
      <link>/unknown/post/raster-time-series-animation/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/unknown/post/raster-time-series-animation/</guid>
      <description>Downloading satelite imageryDownload the data from Sentinel Hub Playground.
library(dplyr)library(lubridate)library(stringr)library(fs)library(purrr)library(glue)library(magick)Extract the dates from the filenamesdates &amp;lt;- tibble(file_paths = fs::dir_ls(&amp;quot;data/2019-12-13-raster-time-series-animation_data/vir/&amp;quot;, glob = &amp;quot;*.jpg&amp;quot;)) %&amp;gt;%mutate(dates = as_date(str_extract(string = file_paths, pattern = regex(&amp;quot; [0-9]+-[0-9]+-[0-9]+&amp;quot;))))dates## # A tibble: 5 x 2## file_paths dates ## &amp;lt;fs::path&amp;gt; &amp;lt;date&amp;gt; ## 1 data/2019-12-13-raster-time-series-animation_data/vir/Sentinel-2 L~ 2015-07-28## 2 data/2019-12-13-raster-time-series-animation_data/vir/Sentinel-2 L~ 2016-07-02## 3 data/2019-12-13-raster-time-series-animation_data/vir/Sentinel-2 L~ 2017-07-02## 4 data/2019-12-13-raster-time-series-animation_data/vir/Sentinel-2 L~ 2018-07-07## 5 data/2019-12-13-raster-time-series-animation_data/vir/Sentinel-2 L~ 2019-07-17Make a gif out of the downloaded satelite imagery.</description>
    </item>
    
    <item>
      <title>test</title>
      <link>/unknown/post/test/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/unknown/post/test/</guid>
      <description>testlibrary(tidyverse)## -- Attaching packages -------------## v ggplot2 3.2.0 v purrr 0.3.2## v tibble 2.1.3 v dplyr 0.8.3## v tidyr 0.8.3 v stringr 1.4.0## v readr 1.3.1 v forcats 0.4.0## -- Conflicts ----------------------## x dplyr::filter() masks stats::filter()## x dplyr::lag() masks stats::lag()tibble::tibble(cars)## # A tibble: 50 x 1## cars$speed $dist## &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;## 1 4 2## 2 4 10## 3 7 4## 4 7 22## 5 8 16## 6 9 10## 7 10 18## 8 10 26## 9 10 34## 10 11 17## # .</description>
    </item>
    
    <item>
      <title>Extract information from raw text (emails) by applying R, friendly regular expressions {rex} and tidy concepts</title>
      <link>/unknown/post/2019-08-25_rex_r/</link>
      <pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/unknown/post/2019-08-25_rex_r/</guid>
      <description>Motivation
Since my main focus here is to learn basics of friendly regular expressions {rex} and processing raw text from emails in a tidy fashion, I will completly neglect any available packages that deal with email data processing like for example REmail.
Secondly, when I first opened the following tutorial where they introduced Python Regex for data scientists applied on the same dataset of Fraudulent Email Corpus from Kaggle, it was a bit intimidating on how much looping and control flowing, unfriendly regular expressions were happening within.</description>
    </item>
    
    <item>
      <title>Prepare to be surprised! Visualizing public data from npd.no</title>
      <link>/unknown/post/2018-10-12_prepare-to-be-surprised-visualizing-public-data-from-npd/</link>
      <pubDate>Sun, 09 Dec 2018 15:13:42 +0000</pubDate>
      
      <guid>/unknown/post/2018-10-12_prepare-to-be-surprised-visualizing-public-data-from-npd/</guid>
      <description>Experimenting with crosstalk, flexdashboard and many more&amp;hellip; Link to the web application
“Visualization can surprise, but it doesn’t scale. Modeling scales well, but it can’t fundamentally surprise.” – Hadley Wickham.
One of the most important phases in the Data Analysis process is data visualization. It is an inherently human activity which relies on our ability to comprehend visual information and make sense of patterns. However, there’s only so much we can spend in visualizing the data, and regardless how many hours we may decide to put into it, it is never going to be enough.</description>
    </item>
    
  </channel>
</rss>